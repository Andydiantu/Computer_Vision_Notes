# Total

**Image Generation**

Convolution mask should be reversed before using
![image.png](image.png)
![image.png](image%201.png)
![image.png](image%202.png)
![image.png](image%203.png)

**Edge Detection**
![image.png](image%204.png)
![image.png](image%205.png)
![image.png](image%206.png)
![image.png](image%207.png)
![image.png](image%208.png)
![image.png](image%209.png)
![image.png](image%2010.png)
![image.png](image%2011.png)
![image.png](image%2012.png)
**As long as the gradient direction are within the threshold, we are grouping them**
![image.png](image%2013.png)

**Fourier Method**
![image.png](image%2014.png)
**Sine components Fs(u) and Cosine components Fc(u)**
![image.png](image%2015.png)
![image.png](image%2016.png)
![image.png](image%2017.png)
![image.png](image%2018.png)
**Convolution operation in spatial domain becomes product operation in frequency domain**
![image.png](image%2019.png)
![image.png](image%2020.png)
**Rotation will only change phase, but not amplitude**
![image.png](image%2021.png)
![image.png](image%2022.png)
![image.png](image%2023.png)
![image.png](image%2024.png)
![image.png](image%2025.png)
![image.png](image%2026.png)
**In the middle is low frequency, on the edge is high frequency**
![image.png](image%2027.png)
![image.png](image%2028.png)

**Representation and Matching**
Cross correlation is comparing a template throughout the picture and check which area has the highest matching, before correlation, we should do preprocessing make sure both image and template has zero mean (shown in third equation)
![image.png](image%2029.png)
Cross correlation and convolution: In cross correlation, the kernel is unflipped, in convolution, the kernel is flipped.
![image.png](image%2030.png)
**For Template matching the complex conjugate of the FT is used, which is different from convolution**
![image.png](image%2031.png)
Shape Discriminants: Shape discriminants are simple object property that can be computed from the image and are independent of the object position
Area: Counting pixel vs Area under edge vector
![image.png](image%2032.png)
Area ⇒ The rectangle around it, Thickness ⇒ The maximum Number of Slice * 2
![image.png](image%2033.png)
Moments
m00 ⇒ Total number of pixels, the area
m10 and m01 ⇒ Used to calculate the center of the rectangle
m20 and m02 ⇒ Measure spread on an axis
![image.png](image%2034.png)
![image.png](image%2035.png)
**Direction of a shape can be calculated by**
![image.png](image%2036.png)
**How pk is calculated**
![image.png](image%2037.png)
![image.png](image%2038.png)
![image.png](image%2039.png)

**Interesting Point Detection**
Desired property of Features
![image.png](image%2040.png)
![image.png](image%2041.png)
Why use corners:
For uniform regions, any changes will result in minimum changes.
For edges, moving along the edge will result in minimum changes.
For corners, any movement will result in significant changes.
Harris ⇒ Detecting changes on the x and y axes
![image.png](image%2042.png)
Direction of Harris
![image.png](image%2043.png)
Two eigenvectors will always be perpendicular.
![image.png](image%2044.png)
![image.png](image%2045.png)
When the corner is rotated, the eigenvalues will not change, but the eigenvectors will change.
![image.png](image%2046.png)
![image.png](image%2047.png)
![image.png](image%2048.png)
LOG is expensive to calculate; we can use DoG to approximate it.
![image.png](image%2049.png)
![image.png](image%2050.png)
![image.png](image%2051.png)

**Feature Descriptors and Matching**
Matching Feature in different images
![image.png](image%2052.png)
Desired property of Feature Descriptor
![image.png](image%2053.png)
![image.png](image%2054.png)
![image.png](image%2055.png)
![image.png](image%2056.png)
![image.png](image%2057.png)
![image.png](image%2058.png)
![image.png](image%2059.png)
![image.png](image%2060.png)
![image.png](image%2061.png)
![image.png](image%2062.png)
![image.png](image%2063.png)
![image.png](image%2064.png)
Feature Matching
1. Directly compare distance with a threshold
2. Match the nearest keypoint in the feature space
3. Using the nearest Neighbour Distance Ratio, compare the distance between the nearest neighbour and second nearest neighbour. If the ratio is around 1, reject; if it is approximately 0, then accept.
![image.png](image%2065.png)

**Feature Tracking**
![image.png](image%2066.png)
Lucas Kanade assumes brightness constancy and temporal persistency.
![image.png](image%2067.png)
For larger displacement, we can use multi-resolution on an image.
![image.png](image%2068.png)
Kalman Filter: A method to estimate movement in the system and correct with actual measurement.
![image.png](image%2069.png)
![image.png](image%2070.png)
![image.png](image%2071.png)
**Added non-linear extension to Kalman filters**
![image.png](image%2072.png)
![image.png](image%2073.png)
![image.png](image%2074.png)
![image.png](image%2075.png)
The reflected light intensity is constant for all directions for Lambertian Surface, and intensity is proportional to the cosine of the angle between the surface normal n.
![image.png](image%2076.png)

**Camera Geometry**
![image.png](image%2077.png)
F is the focal length, which is the distance between the center of projection to the image plane.
![image.png](image%2078.png)
![image.png](image%2079.png)
K is the intrinsic matrix, the former model is the projection matrix, mapping 2D points to 3D.
![image.png](image%2080.png)
R is the rotation matrix, and t represents the displacement from the camera center to the new center.
![image.png](image%2081.png)
The green one represents the relation between camera coordinates and image pixel coordinates (intrinsic matrix) (3D to 2D), the pink one represents the relation between world coordinates and camera coordinates (extrinsic matrix). The blue one combines the two, which is the projection matrix, projecting world coordinates to image coordinates.
![image.png](image%2082.png)
**Geometric properties in 3D and 2D after projection in perspective projection**
![image.png](image%2083.png)
![image.png](image%2084.png)
**Geometric property in orthographic projection**
![image.png](image%2085.png)
![image.png](image%2086.png)
![image.png](image%2087.png)
Camera distortion is put as a parameter in Intrinsic parameters.
![image.png](image%2088.png)

**Computational Stereo**
![image.png](image%2089.png)
![image.png](image%2090.png)
Product of two points is a line, product of two lines is their intersection. X(λ) is the 3D point projected from 2D image plane.
![image.png](image%2091.png)
![image.png](image%2092.png)
![image.png](image%2093.png)
All coordinates are normalized (By times K^-1), then we can simplify the foundational matrix to the essential matrix.
![image.png](image%2094.png)
This is a special case where two cameras have the same camera parameters and are horizontal with the same orientation, making R identity.
![image.png](image%2095.png)
The foundational matrix can also be estimated by solving the constraints by collecting a number of potential matches.
![image.png](image%2096.png)
![image.png](image%2097.png)
**Rectification** makes the two image planes on the same greater plane, making all epipolar lines horizontal, which makes the computation to find matches easier.
![image.png](image%2098.png)
![image.png](image%2099.png)
![image.png](image%20100.png)
![image.png](image%20101.png)

**Photometric Stereo**
![image.png](image%20102.png)
Lambertian surface is the reflected light from all angles are all constant, and it is proportional to the cosine of the incidence angle.
![image.png](image%20103.png)
![image.png](image%20104.png)
We take derivatives of the surface with respect to X and Y; the two vectors are shown in green.
For Photometric Stereo, we have Orthogonal Projection.
![image.png](image%20105.png)
![image.png](image%20106.png)
The above equations are non-linear. A unique solution can be obtained if the equations are linear and independent, and we know the albedo.
![image.png](image%20107.png)
Three different light conditions can help us estimate the p and q for every point in the image.
![image.png](image%20108.png)
![image.png](image%20109.png)
After getting P and Q, integration will give us the depth information of the point. However, this is inefficient as we have to do it for every point.
![image.png](image%20110.png)
![image.png](image%20111.png)
![image.png](image%20112.png)
**Limitation of Shape from Shade method:**
![image.png](image%20113.png)

**Optical Flow**
![image.png](image%20114.png)
Use the assumption where nearby points have the same motion (same u and v).
![image.png](image%20115.png)
u and v are 2D velocities of the object, which are also the only two unknowns.
![image.png](image%20116.png)
![image.png](image%20117.png)
As we have the assumption that nearby pixels have the same velocity, thus the same u and v.
Then use the least square method, similar to Lucas-Kanade, to solve it and get u and v.
![image.png](image%20118.png)
![image.png](image%20119.png)
![image.png](image%20120.png)
![image.png](image%20121.png)
![image.png](image%20122.png)
How to use depth information of one point to estimate the other.
![image.png](image%20123.png)
![image.png](image%20124.png)

**Object Recognition**
![image.png](image%20125.png)
![image.png](image%20126.png)
![image.png](image%20127.png)
Normalizing the patch can make it geometrically consistent, robust, and effective for matching.
![image.png](image%20128.png)
Each descriptor is a point in space; we then use K-means to group them, making them a “word.” Then we can plot a histogram for each image for their “bag of words.”
![image.png](image%20129.png)
![image.png](image%20130.png)
![image.png](image%20131.png)
![image.png](image%20132.png)
Different Models to classify histogram:
- Nearest neighbour
- K-Nearest Neighbours
- SVM (Linear, Non-linear, and multiclass ones)
![image.png](image%20133.png)

**Texture and Region-based Segmentation**
![image.png](image%20134.png)
Histogram methods are vulnerable to noise and lighting effects.
![image.png](image%20135.png)
Check for uniform criteria and stop after no block satisfies the uniform criteria are found.
![image.png](image%20136.png)
![image.png](image%20137.png)
![image.png](image%20138.png)
![image.png](image%20139.png)
![image.png](image%20140.png)
Split the region into two if the criteria are satisfied.
![image.png](image%20141.png)
![image.png](image%20142.png)
P01, 0 describes the direction, and 1 describes the distance. The 0,0 entry should be 4 as there are 4 pairs of 0,0 with the direction and have a distance of 1.
![image.png](image%20143.png)
The co-occurrence matrix should be normalized so each entry becomes the probability of co-occurrence. Therefore, it is independent of the window size.
![image.png](image%20144.png)
![image.png](image%20145.png)
![image.png](image%20146.png)
